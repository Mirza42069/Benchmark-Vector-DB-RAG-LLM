"""
All-in-One RAG Chatbot - Compare All 3 Vector Databases
Pinecone vs PostgreSQL+pgvector vs ChromaDB
"""

import streamlit as st
import os
from dotenv import load_dotenv
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# Vector databases
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain_postgres import PGVector
import chromadb
from langchain_chroma import Chroma

# LangChain
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

# Page config
st.set_page_config(
    page_title="ITS Helpdesk - All Databases",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Configuration
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large")
CHAT_MODEL = os.getenv("CHAT_MODEL", "qwen3:8b")
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "0.75"))
TOP_K = int(os.getenv("TOP_K", "3"))

# Database configs
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "its-helpdesk-chatbot")

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "ragdb")
DB_USER = os.getenv("DB_USER", "raguser")
DB_PASSWORD = os.getenv("DB_PASSWORD", "123")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "its_guidebook")

CHROMA_PATH = "chroma_db"

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}

# Header
st.title("ğŸ¤– ITS Helpdesk Chatbot - All Databases")
st.caption("Compare responses from Pinecone, PostgreSQL+pgvector, and ChromaDB simultaneously")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    st.subheader("ğŸ“‚ Enable Databases")
    use_pinecone = st.checkbox("Pinecone", value=True)
    use_postgresql = st.checkbox("PostgreSQL + pgvector", value=True)
    use_chroma = st.checkbox("ChromaDB", value=True)
    
    if not any([use_pinecone, use_postgresql, use_chroma]):
        st.error("âš ï¸ Select at least one database!")
    
    st.divider()
    
    st.subheader("ğŸ”§ Settings")
    score_threshold = st.slider(
        "Similarity Threshold",
        min_value=0.0,
        max_value=1.0,
        value=SCORE_THRESHOLD,
        step=0.05
    )
    
    top_k = st.slider(
        "Documents per Query",
        min_value=1,
        max_value=10,
        value=TOP_K
    )
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ Clear Chat History", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    st.markdown("""
    ### ğŸ’¡ Sample Questions
    
    **Indonesian:**
    - Bagaimana cara mengubah password myITS Portal?
    - Apa itu Multi-Factor Authentication?
    - Berapa biaya akomodasi per bulan?
    
    **English:**
    - What documents do I need when arriving?
    - How do I register my phone's IMEI?
    - Which banks are available at ITS?
    """)

# Initialize all vector stores
@st.cache_resource
def init_all_vector_stores():
    """Initialize all vector stores"""
    stores = {}
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    
    return stores, embeddings

def get_pinecone_store(embeddings):
    """Initialize Pinecone"""
    try:
        pc = Pinecone(api_key=PINECONE_API_KEY)
        index = pc.Index(INDEX_NAME)
        return PineconeVectorStore(index=index, embedding=embeddings)
    except Exception as e:
        st.sidebar.error(f"âŒ Pinecone: {str(e)[:50]}")
        return None

def get_postgresql_store(embeddings):
    """Initialize PostgreSQL"""
    try:
        connection_string = f"postgresql+psycopg://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
        return PGVector(
            embeddings=embeddings,
            collection_name=COLLECTION_NAME,
            connection=connection_string,
            use_jsonb=True,
        )
    except Exception as e:
        st.sidebar.error(f"âŒ PostgreSQL: {str(e)[:50]}")
        return None

def get_chroma_store(embeddings):
    """Initialize ChromaDB"""
    try:
        client = chromadb.PersistentClient(path=CHROMA_PATH)
        return Chroma(
            client=client,
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings,
        )
    except Exception as e:
        st.sidebar.error(f"âŒ ChromaDB: {str(e)[:50]}")
        return None

def query_database(db_name, vector_store, query, score_threshold, top_k, llm):
    """Query a single database and return results"""
    result = {
        "db_name": db_name,
        "success": False,
        "retrieval_time": 0,
        "llm_time": 0,
        "total_time": 0,
        "docs_found": 0,
        "response": "",
        "sources": [],
        "error": None
    }
    
    try:
        # Retrieval
        start_retrieval = time.time()
        retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": top_k, "score_threshold": score_threshold}
        )
        docs = retriever.invoke(query)
        result["retrieval_time"] = (time.time() - start_retrieval) * 1000
        result["docs_found"] = len(docs)
        
        if docs and len(docs) > 0:
            # Generate response
            docs_text = "\n\n".join(d.page_content for d in docs)
            
            system_prompt = f"""You are a helpful assistant for Institut Teknologi Sepuluh Nopember (ITS) students.
Answer questions based ONLY on the provided context. If the answer is not in the context, say so politely.
Respond in the same language as the question (Indonesian or English).
Keep your response concise and clear.

Context:
{docs_text}"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=query)
            ]
            
            start_llm = time.time()
            response = llm.invoke(messages)
            result["llm_time"] = (time.time() - start_llm) * 1000
            result["response"] = response.content
            result["success"] = True
            
            # Prepare sources
            for doc in docs:
                result["sources"].append({
                    "file": doc.metadata.get('source_file', 'Unknown'),
                    "lang": doc.metadata.get('chunk_language', 'unknown'),
                    "content": doc.page_content[:150] + "..."
                })
        else:
            result["response"] = "No relevant information found in the knowledge base."
            result["success"] = True
        
        result["total_time"] = result["retrieval_time"] + result["llm_time"]
        
    except Exception as e:
        result["error"] = str(e)
        result["response"] = f"Error: {str(e)}"
    
    return result

# Initialize embeddings
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

# Load selected vector stores
with st.spinner("ğŸ”„ Loading databases..."):
    active_stores = {}
    
    if use_pinecone:
        store = get_pinecone_store(embeddings)
        if store:
            active_stores["ğŸŒ² Pinecone"] = store
            st.sidebar.success("âœ… Pinecone")
    
    if use_postgresql:
        store = get_postgresql_store(embeddings)
        if store:
            active_stores["ğŸ˜ PostgreSQL"] = store
            st.sidebar.success("âœ… PostgreSQL")
    
    if use_chroma:
        store = get_chroma_store(embeddings)
        if store:
            active_stores["ğŸ¨ ChromaDB"] = store
            st.sidebar.success("âœ… ChromaDB")

if not active_stores:
    st.error("âŒ No databases available! Please check your configuration.")
    st.stop()

# Display previous messages
for msg in st.session_state.messages:
    if msg["role"] == "user":
        with st.chat_message("user"):
            st.markdown(msg["content"])
    else:
        st.markdown("---")
        st.markdown(f"**Question:** {msg.get('query', '')}")
        
        cols = st.columns(len(msg["responses"]))
        for idx, (db_name, resp) in enumerate(msg["responses"].items()):
            with cols[idx]:
                st.markdown(f"### {db_name}")
                
                # Metrics
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("â±ï¸ Retrieval", f"{resp['retrieval_time']:.0f} ms")
                with col2:
                    st.metric("ğŸ¤– LLM", f"{resp['llm_time']:.0f} ms")
                
                st.metric("ğŸ“„ Docs", resp['docs_found'])
                
                # Response
                if resp["success"]:
                    st.success(resp["response"])
                else:
                    st.error(resp["response"])
                
                # Sources
                if resp["sources"]:
                    with st.expander("ğŸ“š Sources"):
                        for src in resp["sources"]:
                            st.caption(f"**{src['file']}** ({src['lang']})")
                            st.text(src['content'])

# Chat input
if prompt := st.chat_input("Ask me anything about ITS..."):
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    st.markdown("---")
    
    # Initialize LLM
    llm = ChatOllama(model=CHAT_MODEL, temperature=0.1)
    
    # Create columns for each database
    cols = st.columns(len(active_stores))
    results = {}
    
    # Query all databases
    for idx, (db_name, vector_store) in enumerate(active_stores.items()):
        with cols[idx]:
            st.markdown(f"### {db_name}")
            
            with st.spinner(f"ğŸ” Searching..."):
                result = query_database(
                    db_name, vector_store, prompt,
                    score_threshold, top_k, llm
                )
                results[db_name] = result
            
            # Display metrics
            col1, col2 = st.columns(2)
            with col1:
                st.metric("â±ï¸ Retrieval", f"{result['retrieval_time']:.0f} ms")
            with col2:
                st.metric("ğŸ¤– LLM", f"{result['llm_time']:.0f} ms")
            
            st.metric("ğŸ“„ Documents Found", result['docs_found'])
            
            # Display response
            if result["success"]:
                st.success(result["response"])
            else:
                st.error(result["response"])
            
            # Sources
            if result["sources"]:
                with st.expander("ğŸ“š View Sources"):
                    for src in result["sources"]:
                        st.caption(f"**{src['file']}** ({src['lang']})")
                        st.text(src['content'])
                        st.divider()
    
    # Performance comparison
    st.markdown("---")
    st.subheader("ğŸ“Š Performance Comparison")
    
    perf_cols = st.columns(len(results))
    
    # Find fastest
    fastest_retrieval = min(results.values(), key=lambda x: x['retrieval_time'] if x['retrieval_time'] > 0 else float('inf'))
    fastest_total = min(results.values(), key=lambda x: x['total_time'] if x['total_time'] > 0 else float('inf'))
    
    for idx, (db_name, result) in enumerate(results.items()):
        with perf_cols[idx]:
            st.markdown(f"**{db_name}**")
            
            retrieval_label = "ğŸ† " if result == fastest_retrieval else ""
            total_label = "ğŸ† " if result == fastest_total else ""
            
            st.write(f"{retrieval_label}Retrieval: **{result['retrieval_time']:.2f} ms**")
            st.write(f"LLM: **{result['llm_time']:.2f} ms**")
            st.write(f"{total_label}Total: **{result['total_time']:.2f} ms**")
    
    # Save to history
    st.session_state.messages.append({
        "role": "assistant",
        "query": prompt,
        "responses": results
    })

# Footer
st.divider()
st.caption("ğŸ“ ITS Helpdesk Chatbot | All-in-One Comparison | Powered by Ollama")