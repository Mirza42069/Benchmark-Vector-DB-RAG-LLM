"""
Document Ingestion for Pinecone Vector Database
"""

import os
import time
import sys
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain_ollama import OllamaEmbeddings

# Add utils to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from utils.document_processor import DocumentProcessor

load_dotenv()

# Configuration
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "its-helpdesk-chatbot")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "bge-m3")

print("\n" + "="*80)
print("ğŸ“š PINECONE DOCUMENT INGESTION")
print("="*80)

# Validate API key
if not PINECONE_API_KEY:
    print("\nâŒ Error: PINECONE_API_KEY not found in .env file!")
    print("Please add your Pinecone API key to .env file")
    sys.exit(1)

# Initialize embeddings FIRST (for fair benchmark comparison)
print(f"\nğŸ¤– Initializing embedding model: {EMBEDDING_MODEL}")
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

# Initialize Pinecone
print(f"\nğŸ”Œ Connecting to Pinecone...")
pc = Pinecone(api_key=PINECONE_API_KEY)

# Check if index exists
existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

if INDEX_NAME not in existing_indexes:
    print(f"\nğŸ†• Creating new index: {INDEX_NAME}")
    print(f"   Embedding dimension: 1024 (bge-m3)")
    pc.create_index(
        name=INDEX_NAME,
        dimension=1024,  # bge-m3 produces 1024-dimensional vectors
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )
    
    # Wait for index to be ready
    while not pc.describe_index(INDEX_NAME).status["ready"]:
        print("   â³ Waiting for index to be ready...")
        time.sleep(1)
    print(f"   âœ“ Index created successfully")
else:
    print(f"\nâœ… Using existing index: {INDEX_NAME}")

index = pc.Index(INDEX_NAME)

# Check if index already has data
stats = index.describe_index_stats()
existing_count = stats.total_vector_count

if existing_count > 0:
    print(f"\nâœ… Index '{INDEX_NAME}' already has {existing_count} vectors.")
    print("   â­ï¸  Skipping ingestion. Delete index in Pinecone console to re-ingest.")
    
    # Initialize vector store for testing
    vector_store = PineconeVectorStore(index=index, embedding=embeddings)
    
    # Skip to test retrieval
    print("\n" + "="*80)
    print("ğŸ§ª Testing retrieval with sample queries...")
    print("-" * 80)
    
    test_queries = [
        ("ğŸ‡®ğŸ‡©", "Bagaimana cara mengubah password myITS Portal?"),
        ("ğŸ‡¬ğŸ‡§", "What documents do I need to bring when arriving in Surabaya?"),
    ]
    
    for lang_flag, query in test_queries:
        print(f"\n{lang_flag} Testing: \"{query}\"")
        try:
            results = vector_store.similarity_search(query, k=3)
            if results:
                print(f"   âœ… Found {len(results)} relevant chunks")
            else:
                print("   âŒ No results found!")
        except Exception as e:
            print(f"   âŒ Error: {str(e)}")
    
    print("\n" + "="*80)
    print("âœ¨ Ready to use! Run: streamlit run chatbot_pinecone.py")
    print("="*80)
    sys.exit(0)

# Clear existing vectors for fresh start
print("\nğŸ—‘ï¸  Clearing existing vectors...")
try:
    index.delete(delete_all=True)
    print("   âœ“ Cleared existing data")
except:
    print("   â„¹ï¸  No existing data to clear")

# Initialize vector store
vector_store = PineconeVectorStore(index=index, embedding=embeddings)

# Process documents
print("\n" + "="*80)
processor = DocumentProcessor()
chunks = processor.process_documents()

# Clean documents to remove NUL bytes (for consistency with other databases)
print("\nğŸ§¹ Cleaning documents (removing NUL bytes)...")
for chunk in chunks:
    # Remove NUL bytes from content
    chunk.page_content = chunk.page_content.replace('\x00', '')
    
    # Clean metadata strings
    if chunk.metadata:
        for key, value in chunk.metadata.items():
            if isinstance(value, str):
                chunk.metadata[key] = value.replace('\x00', '')

print("   âœ“ Documents cleaned")

# Add documents to Pinecone
print("\n" + "="*80)
print("ğŸ’¾ Adding documents to Pinecone...")

uuids = [f"chunk_{i+1:05d}" for i in range(len(chunks))]

batch_size = 50
total_batches = (len(chunks) + batch_size - 1) // batch_size

print(f"   Processing {len(chunks)} chunks in {total_batches} batches...")

for i in range(0, len(chunks), batch_size):
    batch_docs = chunks[i:i + batch_size]
    batch_ids = uuids[i:i + batch_size]
    
    try:
        vector_store.add_documents(documents=batch_docs, ids=batch_ids)
        current_batch = (i // batch_size) + 1
        print(f"   âœ“ Batch {current_batch}/{total_batches} completed")
    except Exception as e:
        print(f"   âœ— Error in batch {current_batch}: {str(e)}")

# Verify ingestion
print("\nğŸ” Verifying ingestion...")
stats = index.describe_index_stats()
stored_count = stats.total_vector_count

# Calculate language distribution
lang_distribution = {}
for chunk in chunks:
    lang = chunk.metadata.get('chunk_language', 'unknown')
    lang_distribution[lang] = lang_distribution.get(lang, 0) + 1

print("\n" + "="*80)
print("âœ… INGESTION COMPLETED SUCCESSFULLY!")
print("="*80)
print(f"\nğŸ“Š Summary:")
print(f"   â€¢ Total chunks created: {len(chunks)}")
print(f"   â€¢ Vectors stored in Pinecone: {stored_count}")
print(f"   â€¢ Index name: {INDEX_NAME}")
print(f"   â€¢ Embedding model: {EMBEDDING_MODEL}")
print(f"   â€¢ Vector dimension: 1024")
print(f"\nğŸŒ Language Distribution:")
for lang, count in sorted(lang_distribution.items()):
    lang_name = {"id": "ğŸ‡®ğŸ‡© Indonesian", "en": "ğŸ‡¬ğŸ‡§ English", "mixed": "ğŸŒ Mixed"}.get(lang, f"â“ {lang}")
    percentage = (count / len(chunks)) * 100
    print(f"   â€¢ {lang_name}: {count} chunks ({percentage:.1f}%)")

# Test retrieval
print("\n" + "="*80)
print("ğŸ§ª Testing retrieval with sample queries...")
print("-" * 80)

test_queries = [
    ("ğŸ‡®ğŸ‡©", "Bagaimana cara mengubah password myITS Portal?"),
    ("ğŸ‡¬ğŸ‡§", "What documents do I need to bring when arriving in Surabaya?"),
]

for lang_flag, query in test_queries:
    print(f"\n{lang_flag} Testing: \"{query}\"")
    
    try:
        results = vector_store.similarity_search(query, k=3)
        
        if results:
            print(f"   âœ… Found {len(results)} relevant chunks")
            for idx, doc in enumerate(results, 1):
                source = doc.metadata.get('source_file', 'Unknown')
                chunk_lang = doc.metadata.get('chunk_language', '?')
                lang_emoji = {"id": "ğŸ‡®ğŸ‡©", "en": "ğŸ‡¬ğŸ‡§", "mixed": "ğŸŒ"}.get(chunk_lang, "â“")
                preview = doc.page_content[:80].replace('\n', ' ')
                print(f"      {idx}. [{lang_emoji}] {source}: {preview}...")
        else:
            print("   âŒ No results found!")
    except Exception as e:
        print(f"   âŒ Error: {str(e)}")

print("\n" + "="*80)
print("âœ¨ Ready to use! Run: streamlit run chatbot_pinecone.py")
print("="*80)